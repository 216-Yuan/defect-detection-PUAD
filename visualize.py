"""
EfficientAD å¼‚å¸¸æ£€æµ‹å¯è§†åŒ–è„šæœ¬ï¼ˆæ‰¹é‡å…¨ç±»åˆ«æ¨¡å¼ï¼‰

åŠŸèƒ½:
    1. è‡ªåŠ¨æ‰«ææ•°æ®é›†æ ¹ç›®å½•ï¼Œå‘ç°æ‰€æœ‰ç±»åˆ«
    2. æ‰¹é‡åŠ è½½ EfficientAD æ¨¡å‹å¹¶å¯¹æµ‹è¯•é›†è¿›è¡Œæ¨ç†
    3. ç”ŸæˆåŒ…å«åŸå›¾ã€GT Maskã€Heatmapã€Overlay çš„å¯è§†åŒ–å¯¹æ¯”å›¾
    4. ä¿å­˜å¯è§†åŒ–ç»“æœåˆ°åˆ†ç±»åˆ«çš„ç›®å½•ç»“æ„

ç§‘ç ”åŠ¨æœº:
    - æ‰¹é‡å¯è§†åŒ–æ‰€æœ‰ç±»åˆ«ï¼Œå…¨é¢è¯„ä¼°æ¨¡å‹åœ¨ä¸åŒåœºæ™¯ä¸‹çš„è¡¨ç°
    - å¯¹æ¯” Ground Truth å’Œé¢„æµ‹çƒ­åŠ›å›¾ï¼Œå‘ç°æ¨¡å‹çš„ä¼˜åŠ¿å’Œä¸è¶³
    - ä¸ºè®ºæ–‡æ’°å†™æä¾›å…¨é‡çš„å¯è§†åŒ–ç´ æ
    - æ”¯æŒæ¨¡å‹ç¼ºå¤±æ—¶çš„å®¹é”™å¤„ç†ï¼Œä¾¿äºé€æ­¥è®­ç»ƒå’Œæµ‹è¯•
"""

import os
from pathlib import Path

import cv2
import matplotlib.pyplot as plt
import numpy as np
from PIL import Image
import torch
from torchvision import transforms

from puad.dataset import build_dataset, load_ground_truth_masks
from puad.efficientad.inference import load_efficient_ad
from puad.common import build_imagenet_normalization


def denormalize_image(img_tensor: torch.Tensor) -> np.ndarray:
    """åå½’ä¸€åŒ–å›¾åƒï¼Œä»æ ‡å‡†åŒ–å¼ é‡è½¬æ¢ä¸º RGB å›¾åƒæ•°ç»„
    
    å‚æ•°:
        img_tensor: å½’ä¸€åŒ–åçš„å›¾åƒå¼ é‡ (C, H, W)
    
    è¿”å›:
        RGB å›¾åƒæ•°ç»„ (H, W, 3), å€¼èŒƒå›´ [0, 255]
    """
    # ImageNet å½’ä¸€åŒ–å‚æ•°
    mean = np.array([0.485, 0.456, 0.406])
    std = np.array([0.229, 0.224, 0.225])
    
    # è½¬æ¢ä¸º numpy å¹¶åå½’ä¸€åŒ–
    img = img_tensor.cpu().numpy().transpose(1, 2, 0)  # (C, H, W) -> (H, W, C)
    img = img * std + mean  # åå½’ä¸€åŒ–
    img = np.clip(img * 255, 0, 255).astype(np.uint8)  # è½¬æ¢åˆ° [0, 255]
    
    return img


def apply_colormap_on_heatmap(heatmap: np.ndarray) -> np.ndarray:
    """å°†å¼‚å¸¸çƒ­åŠ›å›¾åº”ç”¨ Jet colormap
    
    å‚æ•°:
        heatmap: å¼‚å¸¸çƒ­åŠ›å›¾ (H, W), å€¼èŒƒå›´ [0, 1] æˆ–ä»»æ„
    
    è¿”å›:
        RGB å½©è‰²çƒ­åŠ›å›¾ (H, W, 3), å€¼èŒƒå›´ [0, 255]
    """
    # å½’ä¸€åŒ–åˆ° [0, 255]
    heatmap_normalized = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min() + 1e-8)
    heatmap_uint8 = (heatmap_normalized * 255).astype(np.uint8)
    
    # åº”ç”¨ Jet colormap
    heatmap_colored = cv2.applyColorMap(heatmap_uint8, cv2.COLORMAP_JET)
    heatmap_colored = cv2.cvtColor(heatmap_colored, cv2.COLOR_BGR2RGB)
    
    return heatmap_colored


def create_overlay(original_img: np.ndarray, heatmap: np.ndarray, alpha: float = 0.5) -> np.ndarray:
    """å°†çƒ­åŠ›å›¾å åŠ åˆ°åŸå›¾ä¸Š
    
    å‚æ•°:
        original_img: åŸå§‹ RGB å›¾åƒ (H, W, 3)
        heatmap: å¼‚å¸¸çƒ­åŠ›å›¾ (H, W)
        alpha: çƒ­åŠ›å›¾é€æ˜åº¦ï¼Œ0=å®Œå…¨é€æ˜ï¼Œ1=å®Œå…¨ä¸é€æ˜
    
    è¿”å›:
        å åŠ åçš„ RGB å›¾åƒ (H, W, 3)
    """
    # åº”ç”¨ colormap
    heatmap_colored = apply_colormap_on_heatmap(heatmap)
    
    # å åŠ 
    overlay = cv2.addWeighted(original_img, 1 - alpha, heatmap_colored, alpha, 0)
    
    return overlay


def visualize_sample(
    img_tensor: torch.Tensor,
    anomaly_map: np.ndarray,
    gt_mask: np.ndarray,
    anomaly_score: float,
    save_path: str,
    sample_name: str,
    class_name: str
):
    """ç”Ÿæˆå¹¶ä¿å­˜å•ä¸ªæ ·æœ¬çš„å¯è§†åŒ–å¯¹æ¯”å›¾
    
    å‚æ•°:
        img_tensor: å½’ä¸€åŒ–åçš„å›¾åƒå¼ é‡ (C, H, W)
        anomaly_map: é¢„æµ‹çš„å¼‚å¸¸çƒ­åŠ›å›¾ (H, W)
        gt_mask: Ground Truth æ©ç  (H, W), äºŒå€¼å›¾ {0, 1}
        anomaly_score: å¼‚å¸¸åˆ†æ•°
        save_path: ä¿å­˜è·¯å¾„
        sample_name: æ ·æœ¬åç§°ï¼ˆå¦‚ "000.png"ï¼‰
        class_name: ç±»åˆ«åç§°ï¼ˆå¦‚ "logical_anomalies"ï¼‰
    """
    # åå½’ä¸€åŒ–åŸå›¾
    original_img = denormalize_image(img_tensor)
    
    # åˆ›å»ºå åŠ å›¾
    overlay_img = create_overlay(original_img, anomaly_map, alpha=0.5)
    
    # åº”ç”¨ colormap åˆ°çƒ­åŠ›å›¾
    heatmap_colored = apply_colormap_on_heatmap(anomaly_map)
    
    # åˆ›å»º matplotlib å›¾å½¢
    fig, axes = plt.subplots(2, 2, figsize=(12, 12))
    fig.suptitle(f'{class_name} - {sample_name}\nAnomaly Score: {anomaly_score:.4f}', 
                 fontsize=16, fontweight='bold')
    
    # (1) åŸå›¾
    axes[0, 0].imshow(original_img)
    axes[0, 0].set_title('(1) Original Image', fontsize=12, fontweight='bold')
    axes[0, 0].axis('off')
    
    # (2) GT Mask
    axes[0, 1].imshow(gt_mask, cmap='gray')
    axes[0, 1].set_title('(2) Ground Truth Mask', fontsize=12, fontweight='bold')
    axes[0, 1].axis('off')
    
    # (3) Heatmap
    axes[1, 0].imshow(heatmap_colored)
    axes[1, 0].set_title('(3) Predicted Heatmap (Jet)', fontsize=12, fontweight='bold')
    axes[1, 0].axis('off')
    
    # (4) Overlay
    axes[1, 1].imshow(overlay_img)
    axes[1, 1].set_title('(4) Overlay (Heatmap on Original)', fontsize=12, fontweight='bold')
    axes[1, 1].axis('off')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f"  âœ“ å·²ä¿å­˜: {save_path}")


def discover_categories(dataset_root: str) -> list:
    """è‡ªåŠ¨å‘ç°æ•°æ®é›†æ ¹ç›®å½•ä¸‹çš„æ‰€æœ‰ç±»åˆ«
    
    å‚æ•°:
        dataset_root: æ•°æ®é›†æ ¹ç›®å½•è·¯å¾„
    
    è¿”å›:
        ç±»åˆ«åç§°åˆ—è¡¨
    
    ç§‘ç ”åŠ¨æœº:
        - è‡ªåŠ¨åŒ–å¤„ç†æµç¨‹ï¼Œé¿å…ç¡¬ç¼–ç ç±»åˆ«åç§°
        - æ”¯æŒæ•°æ®é›†æ‰©å±•ï¼Œæ–°å¢ç±»åˆ«æ— éœ€ä¿®æ”¹ä»£ç 
    """
    if not os.path.exists(dataset_root):
        raise ValueError(f"æ•°æ®é›†æ ¹ç›®å½•ä¸å­˜åœ¨: {dataset_root}")
    
    categories = []
    for item in os.listdir(dataset_root):
        item_path = os.path.join(dataset_root, item)
        
        # è¿‡æ»¤æ¡ä»¶ï¼šå¿…é¡»æ˜¯ç›®å½•ï¼Œä¸”åŒ…å« test å­ç›®å½•
        if os.path.isdir(item_path) and not item.startswith('.'):
            test_dir = os.path.join(item_path, "test")
            if os.path.exists(test_dir):
                categories.append(item)
    
    return sorted(categories)


def process_category(
    category: str,
    dataset_root: str,
    model_dir_path: str,
    output_root: str,
    dataset_name: str = "mvtec_loco_anomaly_detection",
    size: str = "s",
    img_size: int = 256,
    max_samples_per_class: int = 10,
    device: str = "cuda"
) -> dict:
    """å¤„ç†å•ä¸ªç±»åˆ«çš„å¯è§†åŒ–
    
    å‚æ•°:
        category: ç±»åˆ«åç§°ï¼ˆå¦‚ "breakfast_box"ï¼‰
        dataset_root: æ•°æ®é›†æ ¹ç›®å½•
        model_dir_path: æ¨¡å‹æ ¹ç›®å½•
        output_root: è¾“å‡ºæ ¹ç›®å½•
        dataset_name: æ•°æ®é›†åç§°
        size: æ¨¡å‹å°ºå¯¸ ("s" or "m")
        img_size: å›¾åƒå°ºå¯¸
        max_samples_per_class: æ¯ä¸ªå¼‚å¸¸å­ç±»åˆ«æœ€å¤šé‡‡æ ·æ•°é‡
        device: è®¡ç®—è®¾å¤‡
    
    è¿”å›:
        ç»Ÿè®¡ä¿¡æ¯å­—å…¸ {"success": bool, "visualized": int, "message": str}
    
    ç§‘ç ”åŠ¨æœº:
        - å°†å•ç±»åˆ«å¤„ç†é€»è¾‘å°è£…ï¼Œä¾¿äºæ‰¹é‡è°ƒç”¨å’Œé”™è¯¯å¤„ç†
        - æ”¯æŒç‹¬ç«‹è¿è¡Œå’Œå®¹é”™ï¼ŒæŸä¸ªç±»åˆ«å¤±è´¥ä¸å½±å“å…¶ä»–ç±»åˆ«
    """
    print(f"\n{'='*60}")
    print(f"ğŸ”¥ æ­£åœ¨å¤„ç†ç±»åˆ«: {category}")
    print(f"{'='*60}")
    
    dataset_path = os.path.join(dataset_root, category)
    category_output_dir = os.path.join(output_root, category)
    
    try:
        # ========== åŠ è½½æ¨¡å‹ ==========
        print(f"ğŸ”§ æ­£åœ¨åŠ è½½ EfficientAD æ¨¡å‹...")
        efficient_ad = load_efficient_ad(
            model_dir_path=model_dir_path,
            size=size,
            dataset_name=dataset_name,
            category=category,
            img_size=img_size,
            device=device
        )
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        
    except FileNotFoundError as e:
        error_msg = f"âš ï¸  æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡ç±»åˆ« {category}"
        print(error_msg)
        print(f"   é”™è¯¯è¯¦æƒ…: {e}")
        return {"success": False, "visualized": 0, "message": error_msg}
    
    except Exception as e:
        error_msg = f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè·³è¿‡ç±»åˆ« {category}"
        print(error_msg)
        print(f"   é”™è¯¯è¯¦æƒ…: {e}")
        return {"success": False, "visualized": 0, "message": error_msg}
    
    try:
        # ========== åŠ è½½æ•°æ®é›† ==========
        print(f"ğŸ“‚ æ­£åœ¨åŠ è½½æµ‹è¯•æ•°æ®é›†...")
        _, _, test_dataset = build_dataset(dataset_path, img_size=img_size)
        print(f"âœ… æµ‹è¯•é›†åŠ è½½æˆåŠŸï¼æ€»æ ·æœ¬æ•°: {len(test_dataset)}")
        
        # ========== åŠ è½½ Ground Truth Masks ==========
        print(f"ğŸ—ºï¸  æ­£åœ¨åŠ è½½ Ground Truth Masks...")
        gt_masks_dict = load_ground_truth_masks(dataset_path, test_dataset, img_size=img_size)
        print(f"âœ… æˆåŠŸåŠ è½½ {len(gt_masks_dict)} ä¸ª GT masks")
        
        # ========== æŒ‰ç±»åˆ«ç»„ç»‡æ ·æœ¬ ==========
        idx_to_class = {i: c for c, i in test_dataset.class_to_idx.items()}
        class_samples = {}  # {class_name: [(sample_idx, img_path, label), ...]}
        
        for sample_idx, (img_path, label) in enumerate(test_dataset.samples):
            class_name = idx_to_class[label]
            if class_name == "good":
                continue  # è·³è¿‡æ­£å¸¸æ ·æœ¬
            
            if class_name not in class_samples:
                class_samples[class_name] = []
            class_samples[class_name].append((sample_idx, img_path, label))
        
        # ========== å¯è§†åŒ–æ¯ä¸ªå­ç±»åˆ«çš„å‰ N ä¸ªæ ·æœ¬ ==========
        print(f"ğŸ¨ å¼€å§‹å¯è§†åŒ–...")
        total_visualized = 0
        
        for class_name, samples in class_samples.items():
            print(f"\nğŸ“Š å¤„ç†å­ç±»åˆ«: {class_name}")
            
            # åˆ›å»ºå­ç±»åˆ«è¾“å‡ºç›®å½•
            class_output_dir = os.path.join(category_output_dir, class_name)
            os.makedirs(class_output_dir, exist_ok=True)
            
            # æŠ½å–å‰ N ä¸ªæ ·æœ¬
            selected_samples = samples[:max_samples_per_class]
            
            for sample_idx, img_path, label in selected_samples:
                # è·å–æ ·æœ¬åç§°
                sample_name = os.path.basename(img_path)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰ GT mask
                if sample_idx not in gt_masks_dict:
                    print(f"  âš ï¸  è·³è¿‡ {sample_name}: æ—  GT mask")
                    continue
                
                # åŠ è½½å›¾åƒ
                img, _ = test_dataset[sample_idx]
                
                # æ¨ç†è·å–å¼‚å¸¸å›¾
                anomaly_score, anomaly_map = efficient_ad.run(img, return_map=True)
                
                # è·å– GT mask
                gt_mask = gt_masks_dict[sample_idx]
                
                # ç”Ÿæˆä¿å­˜è·¯å¾„
                save_filename = os.path.splitext(sample_name)[0] + "_vis.png"
                save_path = os.path.join(class_output_dir, save_filename)
                
                # å¯è§†åŒ–å¹¶ä¿å­˜
                visualize_sample(
                    img_tensor=img,
                    anomaly_map=anomaly_map,
                    gt_mask=gt_mask,
                    anomaly_score=anomaly_score,
                    save_path=save_path,
                    sample_name=sample_name,
                    class_name=class_name
                )
                
                total_visualized += 1
            
            print(f"  âœ… å­ç±»åˆ« {class_name} å®Œæˆï¼Œå¯è§†åŒ– {len(selected_samples)} ä¸ªæ ·æœ¬")
        
        success_msg = f"ç±»åˆ« {category} å®Œæˆï¼Œå…±ç”Ÿæˆ {total_visualized} å¼ å›¾ç‰‡"
        print(f"\nâœ… {success_msg}")
        return {"success": True, "visualized": total_visualized, "message": success_msg}
        
    except Exception as e:
        error_msg = f"âŒ å¤„ç†ç±»åˆ« {category} æ—¶å‡ºé”™"
        print(error_msg)
        print(f"   é”™è¯¯è¯¦æƒ…: {e}")
        return {"success": False, "visualized": 0, "message": error_msg}


def main():
    """ä¸»å‡½æ•°ï¼šæ‰¹é‡å¯è§†åŒ–æ‰€æœ‰ç±»åˆ«çš„ EfficientAD å¼‚å¸¸æ£€æµ‹ç»“æœ"""
    
    # ========== å…¨å±€é…ç½®å‚æ•° ==========
    DATASET_ROOT = r"E:\Dataset\mvtec_loco_anomaly_detection"  # æ•°æ®é›†æ ¹ç›®å½•
    MODEL_DIR_PATH = r"E:\Dataset\mvtec_loco_ad_models"  # æ¨¡å‹æ ¹ç›®å½•
    OUTPUT_ROOT = "vis_results_all"  # è¾“å‡ºæ ¹ç›®å½•
    DATASET_NAME = "mvtec_loco_anomaly_detection"
    SIZE = "s"
    IMG_SIZE = 256
    MAX_IMAGES_PER_TYPE = 10  # æ¯ä¸ªå¼‚å¸¸å­ç±»åˆ«æœ€å¤šé‡‡æ ·æ•°é‡
    
    print("="*60)
    print("ğŸ¨ EfficientAD å¼‚å¸¸æ£€æµ‹æ‰¹é‡å¯è§†åŒ–ï¼ˆå…¨ç±»åˆ«æ¨¡å¼ï¼‰")
    print("="*60)
    print(f"ğŸ“ æ•°æ®é›†æ ¹ç›®å½•: {DATASET_ROOT}")
    print(f"ğŸ¤– æ¨¡å‹æ ¹ç›®å½•: {MODEL_DIR_PATH}")
    print(f"ğŸ’¾ è¾“å‡ºæ ¹ç›®å½•: {OUTPUT_ROOT}")
    print(f"ğŸ”¢ æ¯ç±»é‡‡æ ·ä¸Šé™: {MAX_IMAGES_PER_TYPE}")
    print("="*60)
    
    # ========== è‡ªåŠ¨å‘ç°æ‰€æœ‰ç±»åˆ« ==========
    print("\nğŸ” æ­£åœ¨æ‰«ææ•°æ®é›†ç±»åˆ«...")
    try:
        categories = discover_categories(DATASET_ROOT)
        print(f"âœ… å‘ç° {len(categories)} ä¸ªç±»åˆ«: {categories}")
    except Exception as e:
        print(f"âŒ æ‰«ææ•°æ®é›†å¤±è´¥: {e}")
        return
    
    if len(categories) == 0:
        print("âš ï¸  æœªå‘ç°ä»»ä½•æœ‰æ•ˆç±»åˆ«ï¼Œé€€å‡ºç¨‹åº")
        return
    
    # ========== åˆ›å»ºè¾“å‡ºæ ¹ç›®å½• ==========
    os.makedirs(OUTPUT_ROOT, exist_ok=True)
    
    # ========== ç¡®å®šè®¡ç®—è®¾å¤‡ ==========
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ’» ä½¿ç”¨è®¡ç®—è®¾å¤‡: {device}")
    
    # ========== æ‰¹é‡å¤„ç†æ‰€æœ‰ç±»åˆ« ==========
    print("\nğŸš€ å¼€å§‹æ‰¹é‡å¤„ç†...")
    results = []
    total_visualized = 0
    
    for idx, category in enumerate(categories, 1):
        print(f"\n[{idx}/{len(categories)}]")
        
        result = process_category(
            category=category,
            dataset_root=DATASET_ROOT,
            model_dir_path=MODEL_DIR_PATH,
            output_root=OUTPUT_ROOT,
            dataset_name=DATASET_NAME,
            size=SIZE,
            img_size=IMG_SIZE,
            max_samples_per_class=MAX_IMAGES_PER_TYPE,
            device=device
        )
        
        results.append({"category": category, **result})
        if result["success"]:
            total_visualized += result["visualized"]
    
    # ========== è¾“å‡ºæ±‡æ€»ç»Ÿè®¡ ==========
    print("\n" + "="*60)
    print("ğŸ“Š æ‰¹é‡å¤„ç†å®Œæˆï¼æ±‡æ€»ç»Ÿè®¡ï¼š")
    print("="*60)
    
    success_count = sum(1 for r in results if r["success"])
    failed_count = len(results) - success_count
    
    print(f"âœ… æˆåŠŸå¤„ç†: {success_count}/{len(results)} ä¸ªç±»åˆ«")
    print(f"âŒ å¤±è´¥/è·³è¿‡: {failed_count}/{len(results)} ä¸ªç±»åˆ«")
    print(f"ğŸ¨ æ€»å¯è§†åŒ–å›¾ç‰‡: {total_visualized} å¼ ")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {os.path.abspath(OUTPUT_ROOT)}")
    
    # è¯¦ç»†ç»“æœåˆ—è¡¨
    print(f"\n{'='*60}")
    print("è¯¦ç»†ç»“æœï¼š")
    for r in results:
        status = "âœ…" if r["success"] else "âŒ"
        print(f"  {status} {r['category']}: {r['message']}")
    
    print("="*60)


if __name__ == "__main__":
    main()
