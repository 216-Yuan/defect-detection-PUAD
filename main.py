import argparse
import os

import numpy as np
from puad.common import compute_pro
from puad.dataset import build_dataset, load_ground_truth_masks
from puad.efficientad.inference import load_efficient_ad
from puad.puad import PUAD
import torch

torch.backends.cuda.matmul.allow_tf32 = False
torch.backends.cudnn.allow_tf32 = False

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="PUAD")
    parser.add_argument(
        "dataset_path",
        type=str,
        help="Path to dataset directory containing `train` and `test` (and `validation` in MVTec LOCO AD Dataset)",
    )
    parser.add_argument(
        "model_dir_path",
        type=str,
        help="Path to directory containing pretrained models",
    )
    parser.add_argument(
        "--size",
        choices=["s", "m"],
        type=str,
        default="s",
        help=(
            "Specify the size of EfficientAD used for Picturable anomaly detection "
            "and feature extraction for Unpicturable anomaly detection in either `s` or `m`"
        ),
    )
    parser.add_argument(
        "--feature_extractor",
        choices=["student", "teacher"],
        type=str,
        default="student",
        help=(
            "Specify the network in EfficientAD used for feature extraction for Unpicturable anomaly detection "
            "in either `teacher` or `student`"
        ),
    )
    args = parser.parse_args()

    device = "cuda" if torch.cuda.is_available() else "cpu"

    dataset_dir, category = os.path.split(os.path.abspath(args.dataset_path))
    dataset_name = os.path.split(dataset_dir)[1]
    if not (
        os.path.exists(os.path.join(args.dataset_path, "train"))
        and os.path.exists(os.path.join(args.dataset_path, "test"))
    ):
        raise ValueError("The dataset specified in `dataset_path` must contain `train` and `test` directories.")
    print(f"dataset name : {dataset_name}")
    print(f"category : {category}")
    print(f"size : {args.size}")
    print(f"feature extractor : {args.feature_extractor}")

    # load EfficientAD
    efficient_ad_inference = load_efficient_ad(args.model_dir_path, args.size, dataset_name, category)

    # build dataset
    train_dataset, valid_dataset, test_dataset = build_dataset(args.dataset_path)

    # EfficientAD
    efficient_ad_auroc = efficient_ad_inference.auroc(test_dataset)
    print(f"efficient_ad auroc : {efficient_ad_auroc}")

    # PUAD
    puad = PUAD(feature_extractor=args.feature_extractor)
    puad.load_efficient_ad(efficient_ad_inference)
    puad.train(train_dataset)
    puad.valid(valid_dataset)
    puad_auroc, puad_auroc_for_anomalies = puad.auroc_for_anomalies(test_dataset)

    print(f"puad auroc : {puad_auroc}")
    for anomaly_class, auroc_for_anomaly in puad_auroc_for_anomalies.items():
        print(f"puad auroc for {anomaly_class}: {auroc_for_anomaly}")

    # ============================================================
    # PRO (Per-Region Overlap) è¯„ä¼° - è¯„ä¼°åƒç´ çº§å®šä½èƒ½åŠ›
    # ============================================================
    print("\n" + "="*60)
    print("å¼€å§‹è®¡ç®— PRO æŒ‡æ ‡ï¼ˆPer-Region Overlapï¼‰...")
    print("="*60)
    
    try:
        # åŠ è½½ Ground Truth Masksï¼ˆä»…å¼‚å¸¸æ ·æœ¬ï¼‰
        # ç§‘ç ”è¯´æ˜: PRO æŒ‡æ ‡éœ€è¦åƒç´ çº§ GT æ¥è¯„ä¼°æ¨¡å‹å¯¹å¼‚å¸¸åŒºåŸŸçš„å®šä½èƒ½åŠ›
        gt_masks_dict = load_ground_truth_masks(args.dataset_path, test_dataset)
        
        if len(gt_masks_dict) == 0:
            print("âš ï¸  æœªæ‰¾åˆ°ä»»ä½• Ground Truth Masksï¼Œè·³è¿‡ PRO è®¡ç®—")
        else:
            print(f"âœ“ æˆåŠŸåŠ è½½ {len(gt_masks_dict)} ä¸ªå¼‚å¸¸æ ·æœ¬çš„ Ground Truth Masks")
            
            # æ”¶é›†å¼‚å¸¸æ ·æœ¬çš„é¢„æµ‹å¼‚å¸¸å›¾
            # ç§‘ç ”è¯´æ˜: è¿™é‡Œä½¿ç”¨ EfficientAD çš„å¼‚å¸¸å›¾ä½œä¸ºå®šä½åŸºç¡€
            #          PUAD çš„é©¬æ°è·ç¦»æ˜¯å…¨å±€ç‰¹å¾ï¼Œæ— æ³•ç›´æ¥æ˜ å°„åˆ°åƒç´ ä½ç½®
            anomaly_maps_list = []
            gt_masks_list = []
            
            idx_to_class = {i: c for c, i in test_dataset.class_to_idx.items()}
            
            for sample_idx, (img, label) in enumerate(test_dataset):
                class_name = idx_to_class[label]
                
                # åªå¤„ç†å¼‚å¸¸æ ·æœ¬
                if class_name == "good" or sample_idx not in gt_masks_dict:
                    continue
                
                # è·å–é¢„æµ‹çš„å¼‚å¸¸å›¾ï¼ˆä½¿ç”¨ EfficientADï¼‰
                # æ³¨æ„: è¿™é‡Œæš‚æ—¶ä½¿ç”¨ EfficientAD çš„è¾“å‡ºï¼Œå› ä¸º PUAD çš„å…¨å±€ç‰¹å¾æ— æ³•ç”Ÿæˆåƒç´ çº§çƒ­å›¾
                _, anomaly_map = efficient_ad_inference.run(img, return_map=True)
                
                anomaly_maps_list.append(anomaly_map)
                gt_masks_list.append(gt_masks_dict[sample_idx])
            
            if len(anomaly_maps_list) > 0:
                # è½¬æ¢ä¸º numpy æ•°ç»„
                anomaly_maps_array = np.array(anomaly_maps_list)  # shape: (N, H, W)
                gt_masks_array = np.array(gt_masks_list)          # shape: (N, H, W)
                
                # è®¡ç®— PRO åˆ†æ•°
                pro_score = compute_pro(anomaly_maps_array, gt_masks_array)
                
                print(f"\nğŸ“Š PRO Score (EfficientAD): {pro_score:.4f}")
                print("   (æ³¨: PRO åˆ†æ•°è¶Šé«˜è¡¨ç¤ºå¼‚å¸¸åŒºåŸŸå®šä½èƒ½åŠ›è¶Šå¥½)")
            else:
                print("âš ï¸  æ²¡æœ‰å¯ç”¨çš„å¼‚å¸¸æ ·æœ¬è¿›è¡Œ PRO è®¡ç®—")
                
    except Exception as e:
        print(f"âš ï¸  PRO è®¡ç®—è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print("   è·³è¿‡ PRO è¯„ä¼°ï¼Œç»§ç»­æ‰§è¡Œ...")
    
    print("="*60)
